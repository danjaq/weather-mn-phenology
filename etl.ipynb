{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Weather Enhanced MN Phenology Dataset\n",
    "Combines phenology and weather data at the county level for the state of Minnesota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType, DateType\n",
    "\n",
    "INPUT_FOLDER = \"data/\"\n",
    "OUTPUT_FOLDER = \"out/\"\n",
    "CLIMHEADER=[\"FIPS-YEAR\",\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\",\"JUL\",\\\n",
    "            \"AUG\",\"SEP\",\"OCT\",\"NOV\",\"DEC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Phenology and County Codes\n",
    "Done together since FIPS codes aid with phenology data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data clean up for phenology data\n",
    "- remove empty counties\n",
    "- drop duplicates\n",
    "- align multiple spellings\n",
    "- correct abbreviations, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "County Changes to make based off of mismatched county data\n",
    "- \"ASHLAND\" county is in WI, remove since it's out of scope for this project\n",
    "- \"RAMSEY \" has an extra space to take out\n",
    "- \"ST.LOUIS\" and \"ST. LOUIS\" should be \"ST LOUIS\"\n",
    "- \"BR\" is probably \"Brown\", it's the only \"Br*\" county and there are no counties with \"BR\" as initials\n",
    "- \"MORRISON COUNTY\" should just be \"MORRISON\"\n",
    "- \"AITKIN/ITASCA\" & \"AITKIN /ITASCA\" could be either one, remove for sake of cleanliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processPhenology(spark, phenology):\n",
    "    \"\"\"\n",
    "    Takes a spark session and the data to be read in, performing some data cleanup.\n",
    "    \"\"\"\n",
    "    # read in phenology\n",
    "    mnphn_df = spark.read.option(\"header\", True) \\\n",
    "        .csv(INPUT_FOLDER + phenology)\n",
    "    # Data clean up\n",
    "    mnphn_df = mnphn_df.filter(\"COUNTY is not NULL\").dropDuplicates()\n",
    "    mnphn_df = mnphn_df.filter(\"COUNTY not in ('ASHLAND', 'AITKIN /ITASCA', 'AITKIN/ITASCA')\")\n",
    "    mnphn_df = mnphn_df.withColumn(\"COUNTY\", when(mnphn_df.COUNTY == \"RAMSEY \",\"RAMSEY\")\n",
    "                    .when(mnphn_df.COUNTY == \"ST.LOUIS\",\"ST LOUIS\")\n",
    "                    .when(mnphn_df.COUNTY == \"ST. LOUIS\",\"ST LOUIS\")\n",
    "                    .when(mnphn_df.COUNTY == \"BR\",\"BROWN\")\n",
    "                    .when(mnphn_df.COUNTY == \"MORRISON COUNTY\",\"MORRISON\")\n",
    "                    .otherwise(mnphn_df.COUNTY))\n",
    "    return mnphn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processCounties(spark, counties):\n",
    "    \"\"\"\n",
    "    Takes a spark session and the input data for counties and returns a data frame.\n",
    "    \"\"\"\n",
    "    fips_df = spark.read.json(INPUT_FOLDER + counties, multiLine=True)\n",
    "    # filter to minnesota for coutnies\n",
    "    mnfips_df = fips_df.filter(fips_df.State == \"MN\")\n",
    "    return mnfips_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality Check\n",
    "Compare phenology data's counties to FIPS codes to determine invalid county names. Zero mismatches is a pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def chkPhnCntMismatch(mnphn_df, fips_df):\n",
    "    mismatches = mnphn_df.join(mnfips_df, lower(mnphn_df.COUNTY) == lower(mnfips_df.Name), \"leftanti\") \\\n",
    "        .select(\"COUNTY\").distinct().count()\n",
    "    if mismatches == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Climate Data\n",
    "Percipitation and temperatures per county and month.\n",
    "\n",
    "For perciptation, range of values is listed as 00.00 to 99.99. \"-9.99\" indicates no reading, so should be null. For temperature, the range is -50 to 140 and -99.99 indicates no reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf(returnType=DoubleType()) \n",
    "def noRead2null(obsv, nullVal):\n",
    "    '''\n",
    "    When percipitation is the \"no reading\" value it should be null\n",
    "    '''\n",
    "    if obsv == nullVal:\n",
    "        return None\n",
    "    else:\n",
    "        return obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processClim(spark, clim, noReadVal):\n",
    "    \"\"\"\n",
    "    Takes in the spark session, input climate file, and no reading value,\n",
    "    and and returns a data frame.\n",
    "    \"\"\"\n",
    "    # Climate tables have a variable amount of spaces as their delimiters\n",
    "    # Pandas can handle this where pyspark can't, so use that first\n",
    "    clim_pd = pd.read_csv(INPUT_FOLDER + clim, \\\n",
    "                         sep=\"\\s+\",\\\n",
    "                         names=CLIMHEADER)\n",
    "    clim_df = spark.createDataFrame(clim_pd)\n",
    "    nullVal = lit(noReadVal)\n",
    "    clim_df = clim_df.withColumn(\"JAN\", noRead2null(col(\"JAN\"), nullVal))\\\n",
    "    .withColumn(\"FEB\", noRead2null(col(\"FEB\"), nullVal))\\\n",
    "    .withColumn(\"MAR\", noRead2null(col(\"MAR\"), nullVal))\\\n",
    "    .withColumn(\"APR\", noRead2null(col(\"APR\"), nullVal))\\\n",
    "    .withColumn(\"MAY\", noRead2null(col(\"MAY\"), nullVal))\\\n",
    "    .withColumn(\"JUN\", noRead2null(col(\"JUN\"), nullVal))\\\n",
    "    .withColumn(\"JUL\", noRead2null(col(\"JUL\"), nullVal))\\\n",
    "    .withColumn(\"AUG\", noRead2null(col(\"AUG\"), nullVal))\\\n",
    "    .withColumn(\"SEP\", noRead2null(col(\"SEP\"), nullVal))\\\n",
    "    .withColumn(\"OCT\", noRead2null(col(\"OCT\"), nullVal))\\\n",
    "    .withColumn(\"NOV\", noRead2null(col(\"NOV\"), nullVal))\\\n",
    "    .withColumn(\"DEC\", noRead2null(col(\"DEC\"), nullVal))\n",
    "    return clim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality Check\n",
    "For perciptation, range of values is listed as 00.00 to 99.99, so make sure we don't have any invalid numbers. We can use this same function for temperature as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def chkClimRange(df, low, high):\n",
    "    \"\"\"\n",
    "    Takes a climate data frame and low and high values and checks for invalid entries.\n",
    "    \"\"\"\n",
    "    invalids = df.filter(\"\"\"JAN NOT BETWEEN {lo} AND {hi} OR\n",
    "            FEB NOT BETWEEN {lo} AND {hi} OR\n",
    "            MAR NOT BETWEEN {lo} AND {hi} OR\n",
    "            APR NOT BETWEEN {lo} AND {hi} OR\n",
    "            MAY NOT BETWEEN {lo} AND {hi} OR\n",
    "            JUN NOT BETWEEN {lo} AND {hi} OR\n",
    "            JUL NOT BETWEEN {lo} AND {hi} OR\n",
    "            AUG NOT BETWEEN {lo} AND {hi} OR\n",
    "            SEP NOT BETWEEN {lo} AND {hi} OR\n",
    "            OCT NOT BETWEEN {lo} AND {hi} OR\n",
    "            NOV NOT BETWEEN {lo} AND {hi} OR\n",
    "            DEC NOT BETWEEN {lo} AND {hi}\"\"\".format(lo = low, hi = high)).count()\n",
    "    if invalids == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Analytic Tables\n",
    "Star schema with _Observations_ as the fact table and the following dimension tables:\n",
    "_Biological_, _County_, _Climate_, _Time_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Biological\n",
    "- species\n",
    "- common name\n",
    "- genus\n",
    "- lifeform\n",
    "- group\n",
    "- invasive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createBioTbl(mnphn_df):\n",
    "    \"\"\"\n",
    "    Takes in the phenology data frame and returns the biological table.\n",
    "    \"\"\"\n",
    "    bio_tbl = mnphn_df.select(\\\n",
    "       col(\"SPECIES\").alias(\"species\"),\n",
    "       col(\"SPECIES (COMMON NAME)\").alias(\"common_name\"),\n",
    "       col(\"GENUS \").alias(\"genus\"),\n",
    "       col(\"LIFEFORM\").alias(\"lifeform\"),\n",
    "       col(\"GROUP\").alias(\"group\"),\n",
    "       col(\"INVASIVE\").alias(\"mn_invasive\")).distinct()\n",
    "    return bio_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### County\n",
    "- county_code\n",
    "- county\n",
    "- state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createCntyTbl(mnfips_df):\n",
    "    \"\"\"\n",
    "    Takes in the county data frame to create the county table.\n",
    "    \"\"\"\n",
    "    cnty_tbl = mnfips_df.select(\\\n",
    "        col(\"FIPS\").alias(\"county_code\"),\n",
    "        col(\"Name\").alias(\"county\"),\n",
    "        col(\"State\").alias(\"state\"))\n",
    "    return cnty_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Climate\n",
    "- clim_id\n",
    "- county_code\n",
    "- year\n",
    "- month\n",
    "- tmpMin\n",
    "- tmpMax\n",
    "- tmpAvg\n",
    "- pcpn\n",
    "\n",
    "County code gets a little tricky here since NOAA and everyone else uses different state codes. For this MN restricted set, it's easy to go from 21 => 27, but for a national set, more care would be needed.\n",
    "Also, the climate data is monthly data, so there's no day involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convertClim(df, dataCol):\n",
    "    '''\n",
    "    Takes a climate data frame with month columns and FIPS-YEAR and converts\n",
    "    them into month rows and splits county_code and year.\n",
    "    Thanks to Doug on this post for the method: https://stackoverflow.com/a/64130519\n",
    "    '''\n",
    "    df = df.withColumn(\"county_code\", col(\"FIPS-YEAR\").substr(1,5))\\\n",
    "    .withColumn(\"year\", col(\"FIPS-YEAR\").substr(8,4)).drop(\"FIPS-YEAR\")\n",
    "\n",
    "    df = df.withColumn('j1', array(lit('01'), col('JAN')))\n",
    "    df = df.withColumn('f2', array(lit('02'), col('FEB')))\n",
    "    df = df.withColumn('m3', array(lit('03'), col('MAR')))\n",
    "    df = df.withColumn('a4', array(lit('04'), col('APR')))\n",
    "    df = df.withColumn('m5', array(lit('05'), col('MAY')))\n",
    "    df = df.withColumn('j6', array(lit('06'), col('JUN')))\n",
    "    df = df.withColumn('j7', array(lit('07'), col('JUL')))\n",
    "    df = df.withColumn('a8', array(lit('08'), col('AUG')))\n",
    "    df = df.withColumn('s9', array(lit('09'), col('SEP')))\n",
    "    df = df.withColumn('o10', array(lit('10'), col('OCT')))\n",
    "    df = df.withColumn('n11', array(lit('11'), col('NOV')))\n",
    "    df = df.withColumn('d12', array(lit('12'), col('DEC')))\n",
    "\n",
    "    df = df.withColumn('months', array('j1','f2','m3','a4','m5','j6','j7','a8','s9','o10','n11','d12'))\n",
    "\n",
    "    df = df.withColumn('months', explode('months'))\n",
    "\n",
    "    df = df.withColumn('month', col('months')[0])\n",
    "    df = df.withColumn(dataCol, col('months')[1])\n",
    "    return df.select(\"county_code\", \"year\", \"month\", dataCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createClimTbls(pcpn_df, tmpMin_df, tmpMax_df, tmpAvg_df):\n",
    "    \"\"\"\n",
    "    Takes in the climate data frames to create the climate table.\n",
    "    \"\"\"\n",
    "    # filter to minnesota and change FIPS code\n",
    "    pcpn_tbl = pcpn_df.filter(col(\"FIPS-YEAR\").like('21%'))\\\n",
    "        .withColumn(\"FIPS-YEAR\", ((col(\"FIPS-YEAR\")\\\n",
    "        .cast('bigint')+6000000000).cast('string')))\n",
    "    tmpMin_tbl = tmpMin_df.filter(col(\"FIPS-YEAR\").like('21%'))\\\n",
    "        .withColumn(\"FIPS-YEAR\", ((col(\"FIPS-YEAR\")\\\n",
    "        .cast('bigint')+6000000000).cast('string')))\n",
    "    tmpMax_tbl = tmpMax_df.filter(col(\"FIPS-YEAR\").like('21%'))\\\n",
    "        .withColumn(\"FIPS-YEAR\", ((col(\"FIPS-YEAR\")\\\n",
    "        .cast('bigint')+6000000000).cast('string')))\n",
    "    tmpAvg_tbl = tmpAvg_df.filter(col(\"FIPS-YEAR\").like('21%'))\\\n",
    "        .withColumn(\"FIPS-YEAR\", ((col(\"FIPS-YEAR\")\\\n",
    "        .cast('bigint')+6000000000).cast('string')))\n",
    "    # run the convert function on our four climate tables\n",
    "    pcpn_tbl = convertClim(pcpn_tbl, 'pcpn')\n",
    "    tmpMin_tbl = convertClim(tmpMin_tbl, 'tmpMin')\n",
    "    tmpMax_tbl = convertClim(tmpMax_tbl, 'tmpMax')\n",
    "    tmpAvg_tbl = convertClim(tmpAvg_tbl, 'tmpAvg')\n",
    "    return pcpn_tbl, tmpMin_tbl, tmpMax_tbl, tmpAvg_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def joinClimTbls(pcpn_tbl, tmpMin_tbl, tmpMax_tbl, tmpAvg_tbl):\n",
    "    \"\"\"\n",
    "    Takes in the individual climate tables and joins them.\n",
    "    \"\"\"\n",
    "    # Combine all four climate tables\n",
    "    clim_tbl = pcpn_tbl.join(tmpMin_tbl, [\"county_code\", \"year\", \"month\"])\n",
    "    clim_tbl = clim_tbl.join(tmpMax_tbl, [\"county_code\", \"year\", \"month\"])\n",
    "    clim_tbl = clim_tbl.join(tmpAvg_tbl, [\"county_code\", \"year\", \"month\"])\n",
    "    #add id column and set data types\n",
    "    clim_tbl = clim_tbl.withColumn(\"clim_id\", monotonically_increasing_id())\\\n",
    "        .withColumn(\"year\", col(\"year\").cast('int'))\\\n",
    "        .withColumn(\"month\", col(\"month\").cast('int'))\\\n",
    "        .withColumn(\"pcpn\", col(\"pcpn\").cast('double'))\\\n",
    "        .withColumn(\"tmpMin\", col(\"tmpMin\").cast('double'))\\\n",
    "        .withColumn(\"tmpMax\", col(\"tmpMax\").cast('double'))\\\n",
    "        .withColumn(\"tmpAvg\", col(\"tmpAvg\").cast('double'))\n",
    "    return clim_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality Check\n",
    "Ensure the joining was succesfull.\n",
    "Row counts for all the tables should be the same and a look at a random entry to spot check validity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def chkClimJoin(clim_tbl, pcpn_tbl, tmpMin_tbl, tmpMax_tbl, tmpAvg_tbl):\n",
    "    \"\"\"\n",
    "    Takes a random row out of the climate table and confirms the values match\n",
    "    against the cliamte data frames.\n",
    "    \"\"\"\n",
    "    randRow = clim_tbl.sample(False, 0.0001).first()\n",
    "    # print(randRow)\n",
    "    pcpn_match = pcpn_tbl.select(col(\"pcpn\")).where((col(\"county_code\") == randRow[\"county_code\"])\\\n",
    "                                       & (col(\"year\") == randRow[\"year\"])\\\n",
    "                                       & (col(\"month\") == randRow[\"month\"]))\n",
    "    tmpMin_match = tmpMin_tbl.select(col(\"tmpMin\")).where((col(\"county_code\") == randRow[\"county_code\"])\\\n",
    "                                       & (col(\"year\") == randRow[\"year\"])\\\n",
    "                                       & (col(\"month\") == randRow[\"month\"]))\n",
    "    tmpMax_match = tmpMax_tbl.select(col(\"tmpMax\")).where((col(\"county_code\") == randRow[\"county_code\"])\\\n",
    "                                       & (col(\"year\") == randRow[\"year\"])\\\n",
    "                                       & (col(\"month\") == randRow[\"month\"]))\n",
    "    tmpAvg_match = tmpAvg_tbl.select(col(\"tmpAvg\")).where((col(\"county_code\") == randRow[\"county_code\"])\\\n",
    "                                       & (col(\"year\") == randRow[\"year\"])\\\n",
    "                                       & (col(\"month\") == randRow[\"month\"]))\n",
    "    # print(float(pcpn_match.first()[\"pcpn\"]), float(tmpMin_match.first()[\"tmpMin\"]), float(tmpMax_match.first()[\"tmpMax\"]), float(tmpAvg_match.first()[\"tmpAvg\"]))\n",
    "    # All *match should be one row and match the randRow.\n",
    "    if (pcpn_match.count() != 1)\\\n",
    "        or (tmpMin_match.count() != 1)\\\n",
    "        or (tmpMax_match.count() != 1)\\\n",
    "        or (tmpAvg_match.count() != 1):\n",
    "        raise Exception(\"Join failed, too many entries.\")\n",
    "        return False\n",
    "    elif (randRow[\"pcpn\"] != float(pcpn_match.first()[\"pcpn\"]))\\\n",
    "            or (randRow[\"tmpMin\"] != float(tmpMin_match.first()[\"tmpMin\"]))\\\n",
    "            or (randRow[\"tmpMax\"] != float(tmpMax_match.first()[\"tmpMax\"]))\\\n",
    "            or (randRow[\"tmpAvg\"] != float(tmpAvg_match.first()[\"tmpAvg\"])):\n",
    "        raise Exception(\"Join failed, mismatched data.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Observations (fact table)\n",
    "- obsv_id\n",
    "- date\n",
    "- species\n",
    "- county_code\n",
    "- clim_id\n",
    "- event\n",
    "Join in the county data to the phenology set, so we can select it back out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createObsvTbl(mnphn_df, mnfips_df, clim_tbl):\n",
    "    \"\"\"\n",
    "    Takes in the phenology and counties data frames and the climate table\n",
    "    to return the observation table.\n",
    "    \"\"\"\n",
    "    mnphn_df = mnphn_df.join(mnfips_df, lower(mnphn_df.COUNTY) == lower(mnfips_df.Name))\n",
    "    obsv_tbl = mnphn_df.select(monotonically_increasing_id().alias(\"obsv_id\"), \\\n",
    "        from_unixtime(unix_timestamp(\\\n",
    "            concat_ws(\"-\",col(\"YEAR\"),col(\"DAY\")), 'yyyy-dd-MMM')).cast(\"date\").alias(\"date\"), \\\n",
    "        col(\"YEAR\").cast(\"int\").alias(\"year\"), \\\n",
    "        from_unixtime(unix_timestamp(col(\"DAY\"), 'dd-MMM'), 'MM').cast(\"int\").alias(\"month\"), \\\n",
    "        col(\"SPECIES\").alias(\"species\"), \\\n",
    "        col(\"FIPS\").alias(\"county_code\"), \\\n",
    "        col(\"EVENT\").alias(\"event\"))\n",
    "    # join with the climate table to extract the clim_ids.\n",
    "    # this also has the effect of pruning poorly formatted dates from phenology\n",
    "    obsv_tbl = obsv_tbl.join(clim_tbl, [\"county_code\", \"year\", \"month\"])\n",
    "    obsv_tbl = obsv_tbl.select(\"obsv_id\", \"date\", \"species\", \"county_code\", \"clim_id\", \"event\")\n",
    "    return obsv_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Time Table\n",
    "The most specific time item is date from observations, so go from there.\n",
    "- date\n",
    "- day\n",
    "- week\n",
    "- month\n",
    "- year\n",
    "- weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createTimeTbl(obsv_tbl):\n",
    "    \"\"\"\n",
    "    Takes the observations table and creates the time table on the same scale.\n",
    "    \"\"\"\n",
    "    # create datetime column from original timestamp column\n",
    "    time_tbl = obsv_tbl.select('date').distinct()\n",
    "\n",
    "    # extract columns to create time table\n",
    "    time_tbl = time_tbl.select(\n",
    "        col(\"date\"),\n",
    "        dayofmonth(col(\"date\")).alias(\"day\"),\n",
    "        weekofyear(col(\"date\")).alias(\"week\"),\n",
    "        month(col(\"date\")).alias(\"month\"),\n",
    "        year(col(\"date\")).alias(\"year\"),\n",
    "        date_format(col(\"date\"), \"E\").alias(\"weekday\")\n",
    "        )\n",
    "\n",
    "    return time_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Write out Analysis Tables to Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def writeTables():\n",
    "    \"\"\"\n",
    "    Writes out the analysis tables into parquet files.\n",
    "    \"limit(1000)\" is added due to size constraints on the Udacity Workspace.\n",
    "    \"\"\"\n",
    "    obsv_tbl.limit(1000).write.partitionBy(\"date\", \"county_code\").mode(\"overwrite\")\\\n",
    "        .parquet(OUTPUT_FOLDER + \"obsv.parquet\")\n",
    "    bio_tbl.limit(1000).write.mode(\"overwrite\")\\\n",
    "        .parquet(OUTPUT_FOLDER + \"bio.parquet\")\n",
    "    cnty_tbl.limit(1000).write.partitionBy(\"state\").mode(\"overwrite\")\\\n",
    "        .parquet(OUTPUT_FOLDER + \"cnty.parquet\")\n",
    "    clim_tbl.limit(1000).write.partitionBy(\"year\", \"county_code\").mode(\"overwrite\")\\\n",
    "        .parquet(OUTPUT_FOLDER + \"clim.parquet\")\n",
    "    time_tbl.limit(1000).write.partitionBy(\"year\").mode(\"overwrite\")\\\n",
    "        .parquet(OUTPUT_FOLDER + \"time.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Run ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Extract and clean-up on Phenology and Counties.\n",
    "Returns True for county mismatch data quality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "mnphn_df = processPhenology(spark, \"2018.4.6-MNPN-WEBSITE-master-dataset.csv\")\n",
    "mnfips_df = processCounties(spark, \"USDA-NRCS_FIPS_Codes.json\")\n",
    "print(chkPhnCntMismatch(mnphn_df, mnfips_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Extract and clean-up for Percipitation and temperature data. Returns true for a passing check of invalid measurements outside of the appropriate ranges (0-99.99 for percipitation and -50 to 140 for temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pcpn_df = processClim(spark, \"climdiv-pcpncy-v1.0.0-20211104\", -9.99)\n",
    "print(chkClimRange(pcpn_df, 0, 99.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tmpMax_df = processClim(spark, \"climdiv-tmaxcy-v1.0.0-20211104\", -99.99)\n",
    "print(chkClimRange(pcpn_df, -50, 140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tmpMin_df = processClim(spark, \"climdiv-tmincy-v1.0.0-20211104\", -99.99)\n",
    "print(chkClimRange(pcpn_df, -50, 140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tmpAvg_df = processClim(spark, \"climdiv-tmpccy-v1.0.0-20211104\", -99.99)\n",
    "print(chkClimRange(pcpn_df, -50, 140))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create analytics tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "bio_tbl = createBioTbl(mnphn_df)\n",
    "cnty_tbl = createCntyTbl(mnfips_df)\n",
    "pcpn_tbl, tmpMin_tbl, tmpMax_tbl, tmpAvg_tbl \\\n",
    "    = createClimTbls(pcpn_df, tmpMin_df, tmpMax_df, tmpAvg_df)\n",
    "clim_tbl = joinClimTbls(pcpn_tbl, tmpMin_tbl, tmpMax_tbl, tmpAvg_tbl)\n",
    "obsv_tbl = createObsvTbl(mnphn_df, mnfips_df, clim_tbl)\n",
    "time_tbl = createTimeTbl(obsv_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Data Qualtiy check on climate table join\n",
    "print(chkClimJoin(clim_tbl, pcpn_tbl, tmpMin_tbl, tmpMax_tbl, tmpAvg_tbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "writeTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sample Query\n",
    "A query to examing percipitation in the month of the last flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "obsv_tbl.createOrReplaceTempView(\"OBSV\")\n",
    "bio_tbl.createOrReplaceTempView(\"BIO\")\n",
    "cnty_tbl.createOrReplaceTempView(\"CNTY\")\n",
    "clim_tbl.createOrReplaceTempView(\"CLIM\")\n",
    "time_tbl.createOrReplaceTempView(\"TIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pcpn</th>\n",
       "      <th>county</th>\n",
       "      <th>event</th>\n",
       "      <th>common_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>AMERICAN TOAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>SNOWSHOE HARES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>YELLOW BILLED CUCKOO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987-07-03</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>WHITE WATER LILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>NEW JERSEY TEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>CUT LEAVED WATER HOREHOUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>WHITE PRAIRIE CLOVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1987-07-03</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>BUTTERFLY WEED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>WATER HOREHOUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1987-07-23</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Hennepin</td>\n",
       "      <td>LAST FLOWER</td>\n",
       "      <td>YELLOW BELLIED CUCKOO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  pcpn    county        event                 common_name\n",
       "0  1987-07-23  10.1  Hennepin  LAST FLOWER               AMERICAN TOAD\n",
       "1  1987-07-23  10.1  Hennepin  LAST FLOWER              SNOWSHOE HARES\n",
       "2  1987-07-23  10.1  Hennepin  LAST FLOWER        YELLOW BILLED CUCKOO\n",
       "3  1987-07-03  10.1  Hennepin  LAST FLOWER            WHITE WATER LILY\n",
       "4  1987-07-23  10.1  Hennepin  LAST FLOWER              NEW JERSEY TEA\n",
       "5  1987-07-23  10.1  Hennepin  LAST FLOWER  CUT LEAVED WATER HOREHOUND\n",
       "6  1987-07-23  10.1  Hennepin  LAST FLOWER        WHITE PRAIRIE CLOVER\n",
       "7  1987-07-03  10.1  Hennepin  LAST FLOWER              BUTTERFLY WEED\n",
       "8  1987-07-23  10.1  Hennepin  LAST FLOWER             WATER HOREHOUND\n",
       "9  1987-07-23  10.1  Hennepin  LAST FLOWER       YELLOW BELLIED CUCKOO"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT o.date,\n",
    "        c.pcpn,\n",
    "        cy.county,\n",
    "        o.event,\n",
    "        b.common_name\n",
    "        FROM OBSV o\n",
    "        JOIN CLIM c\n",
    "        ON o.clim_id == c.clim_id\n",
    "        JOIN BIO b\n",
    "        ON o.species == b.species\n",
    "        JOIN CNTY cy\n",
    "        ON o.county_code == cy.county_code\n",
    "        WHERE o.event == 'LAST FLOWER' AND\n",
    "        cy.county == 'Hennepin'\n",
    "        ORDER BY c.pcpn DESC\n",
    "        \"\"\").limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
